{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba94ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import openpyxl\n",
    "from openpyxl import Workbook,load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5bd0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      URL_ID                                                URL\n",
      "0      123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
      "1      321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
      "2     2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
      "3     4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
      "4      432.0  https://insights.blackcoffer.com/rise-of-telem...\n",
      "..       ...                                                ...\n",
      "109  50921.0  https://insights.blackcoffer.com/coronavirus-i...\n",
      "110  51382.8  https://insights.blackcoffer.com/coronavirus-i...\n",
      "111  51844.6  https://insights.blackcoffer.com/what-are-the-...\n",
      "112  52306.4  https://insights.blackcoffer.com/marketing-dri...\n",
      "113  52768.2  https://insights.blackcoffer.com/continued-dem...\n",
      "\n",
      "[114 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_excel('input.xlsx')\n",
    "print(df)\n",
    "stop_words = []\n",
    "final_stop_words = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad0231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path where the text files are located\n",
    "dir = 'C:\\\\Users\\\\irzak\\\\ml\\\\intern'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af372b31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m URL \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m][ind]\n\u001b[0;32m     27\u001b[0m url \u001b[38;5;241m=\u001b[39m URL\n\u001b[1;32m---> 29\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html\u001b[38;5;241m.\u001b[39mcontent,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     34\u001b[0m title \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1045\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1046\u001b[0m         (\n\u001b[0;32m   1047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1053\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m    360\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m     sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the glob module to retrieve a list of text files in the directory\n",
    "file_paths = glob.glob(dir + '/*.txt')\n",
    "\n",
    "for f in file_paths:\n",
    "    with open(f, 'r') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()\n",
    "        stop_words.extend(words)\n",
    "\n",
    "for x in stop_words:\n",
    "    final_stop_words.append(x.lower())\n",
    "\n",
    "final_stop_words = set(final_stop_words)\n",
    "def remove_unicode(scrapped_data):\n",
    "    no_unicode = []\n",
    "    for i in scrapped_data:\n",
    "        strencode = i.encode(\"ascii\", \"ignore\")\n",
    "        strdecode = strencode.decode()\n",
    "        no_unicode.append(strdecode)\n",
    "        \n",
    "    return no_unicode\n",
    "\n",
    "for ind in df.index:\n",
    "    \n",
    "    URL_ID = df['URL_ID'][ind]\n",
    "    URL = df['URL'][ind]\n",
    "    url = URL\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(html.content,'html.parser') \n",
    "    \n",
    "   \n",
    "    title = soup.find_all('h1')\n",
    "    result_1 = soup.find('div',class_=['td-post-content', 'tagdiv-type'])\n",
    "    \n",
    "    if result_1 != None and len(title) >= 1:     \n",
    "        \n",
    "        para = result_1.find_all('p')\n",
    "        heading = str(title[0].text)\n",
    "        \n",
    "        paragraph=\"\"\n",
    "       \n",
    "        for p in para:\n",
    "            paragraph = paragraph + str(p.text)\n",
    "            \n",
    "        \n",
    "        scrapped_data = heading + paragraph\n",
    "        scrapped_data = scrapped_data.split()\n",
    "        scrapped_data = remove_unicode(scrapped_data)\n",
    "        scrapped_data = ' '.join(scrapped_data)\n",
    "        \n",
    "        \n",
    "        file_name = str(URL_ID)+'.txt'\n",
    "        with open('C:\\\\Users\\\\irzak\\\\ml\\\\intern\\\\extracted\\\\'+file_name,'w') as file:\n",
    "            words = scrapped_data.split()\n",
    "            for word in words:\n",
    "                file.write(word + ' ')\n",
    "                    \n",
    "                    \n",
    "        file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1bf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fun(extracted_str):\n",
    "    text = extracted_str\n",
    "    tkn = word_tokenize(text)\n",
    "    return tkn\n",
    "def tokenize_fun_sent(extracted_str):\n",
    "    text = extracted_str\n",
    "    tkn = sent_tokenize(text)\n",
    "    return tkn\n",
    "def filter_stopwords(word_tokens):\n",
    "    filtered_data = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_data.append(w)\n",
    "            \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0538f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = set()\n",
    "with open('C:\\\\Users\\\\irzak\\\\ml\\\\intern\\\\MasterDictionary\\\\positive-words.txt','r') as file:   \n",
    "    for line in file:       \n",
    "        for word in line.split():    \n",
    "            positive_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f313de",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = set()\n",
    "with open('C:\\\\Users\\\\irzak\\\\ml\\\\intern\\\\MasterDictionary\\\\negative-words.txt','r') as file:   \n",
    "    for line in file:       \n",
    "        for word in line.split():    \n",
    "            negative_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e34160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_symbols(filtered_data):\n",
    "    english_words = []\n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    for i in filtered_data:\n",
    "        i = \" \".join(w for w in nltk.wordpunct_tokenize(i) if w.lower() in words or not w.isalpha())\n",
    "        english_words.append(i);\n",
    "        \n",
    "    english_words_no_punctuations = []\n",
    "    for i in english_words:\n",
    "        i = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        english_words_no_punctuations.append(i)\n",
    "    \n",
    "    no_unicode = []\n",
    "    for i in english_words_no_punctuations:\n",
    "        strencode = i.encode(\"ascii\", \"ignore\")\n",
    "        strdecode = strencode.decode()\n",
    "        no_unicode.append(strdecode)\n",
    "    \n",
    "    english_words_no_punctuations = no_unicode    \n",
    "    final_filtered_data = set(english_words_no_punctuations)\n",
    "    final_filtered_data = list(final_filtered_data)\n",
    "    for i in final_filtered_data:\n",
    "        if i=='':\n",
    "            final_filtered_data.remove(i)\n",
    "        \n",
    "    return final_filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9408255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_inbuilt_stopwords_punctuations(word_tokens):\n",
    "    inbuilt_stopwords = stopwords.words('english')\n",
    "    \n",
    "    data_no_stopwords = []\n",
    "    for item in word_tokens:\n",
    "        if item not in inbuilt_stopwords:\n",
    "            data_no_stopwords.append(item)\n",
    "            \n",
    "    no_pun_stop = []\n",
    "    for i in data_no_stopwords:\n",
    "        i = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        no_pun_stop.append(i)\n",
    "        \n",
    "    return no_pun_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaec898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\") or word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "        count -= 1\n",
    "        \n",
    "    if word.endswith(\"ted\") or word.endswith(\"tes\") or word.endswith(\"ses\") or word.endswith(\"ied\") or word.endswith(\"ies\"):\n",
    "        count += 1\n",
    "                \n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0db5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = pd.DataFrame(columns=['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX_WORD_COUNT','WORD_COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7365dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found for URL_ID 11668.0\n",
      "File not found for URL_ID 17671.4\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the number of syllables in a word\n",
    "def syllable_count(word):\n",
    "    # Your syllable counting logic here (you may use a library like Pyphen)\n",
    "    # This is a simple example that assumes each vowel contributes one syllable\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            count += 1\n",
    "    return max(count, 1)\n",
    "\n",
    "# Function to calculate the FOG index\n",
    "def calculate_fog_index(average_sentence_length, percentage_complex_words):\n",
    "    return 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Define the directory path where the text files are located\n",
    "directory = r'C:\\Users\\irzak\\ml\\intern\\extracted'\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame(columns=[\n",
    "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', \n",
    "    'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', \n",
    "    'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', \n",
    "    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "])\n",
    "\n",
    "# Assuming you have a DataFrame df with 'URL_ID' and 'URL' columns\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Construct the file path based on URL_ID\n",
    "    file_path = os.path.join(directory, f\"{url_id}.txt\")\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found for URL_ID {url_id}\")\n",
    "        continue\n",
    "\n",
    "    # Read the content of the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "\n",
    "    # Tokenize the text into words and sentences\n",
    "    word_tokens = word_tokenize(text_content)\n",
    "    sentence_tokens = sent_tokenize(text_content)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in word_tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "    # Calculate scores based on your requirements\n",
    "    positive_score = sum(1 for word in filtered_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in filtered_words if word in negative_words)\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / len(word_tokens) + 0.000001\n",
    "\n",
    "    average_sentence_length = len(word_tokens) / len(sentence_tokens)\n",
    "    percentage_complex_words = sum(1 for word in filtered_words if syllable_count(word) > 2) / len(word_tokens)\n",
    "    \n",
    "    fog_index = calculate_fog_index(average_sentence_length, percentage_complex_words)\n",
    "    \n",
    "    average_no_of_words_per_sentence = len(word_tokens) / len(sentence_tokens)\n",
    "    \n",
    "    complex_word_count = sum(1 for word in filtered_words if syllable_count(word) > 2)\n",
    "    word_count = len(filtered_words)\n",
    "    \n",
    "    syllable_per_word = sum(syllable_count(word) for word in filtered_words) / len(filtered_words)\n",
    "    \n",
    "    personal_pronouns = len(re.findall(r'\\b(he|she|it|we|they|you|I)\\b', text_content, re.IGNORECASE))\n",
    "    \n",
    "    total_characters = sum(len(word) for word in filtered_words)\n",
    "    average_word_length = total_characters / len(filtered_words)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    row_data = {\n",
    "    'URL_ID': url_id,\n",
    "    'URL': url,\n",
    "    'POSITIVE SCORE': positive_score,\n",
    "    'NEGATIVE SCORE': negative_score,\n",
    "    'POLARITY SCORE': polarity_score,\n",
    "    'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "    'AVG SENTENCE LENGTH': average_sentence_length,\n",
    "    'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "    'FOG INDEX': fog_index,\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE': average_no_of_words_per_sentence,\n",
    "    'COMPLEX WORD COUNT': complex_word_count,\n",
    "    'WORD COUNT': word_count,\n",
    "    'SYLLABLE PER WORD': syllable_per_word,\n",
    "    'PERSONAL PRONOUNS': personal_pronouns,\n",
    "    'AVG WORD LENGTH': average_word_length\n",
    "}\n",
    "\n",
    "    result_df = pd.concat([result_df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    result_df.to_excel('output_results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72238c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2621717190.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    jupyter nbconvert --to script your_notebook.ipynb\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter nbconvert --to script your_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6224e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
